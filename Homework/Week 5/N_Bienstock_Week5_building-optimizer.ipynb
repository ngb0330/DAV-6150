{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing our Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll build out the component that finds the hypothesis function that minimizes the output of our loss component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so we'll need to create some Hypothesis instances.  Copy and paste the Hypothesis class from the previous lab here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis\n",
    "class Hypothesis:\n",
    "    def __init__(self, coef_, intercept_, x_values):\n",
    "        self.coef_ = coef_\n",
    "        self.intercept_ = intercept_\n",
    "        self.x_values = x_values\n",
    "    \n",
    "    def predict_value(self, input_value):\n",
    "        return self.coef_*input_value + self.intercept_\n",
    "    \n",
    "    def predict(self):\n",
    "        return [self.predict_value(x_value) for x_value in self.x_values]\n",
    "    \n",
    "    def trace(self, mode = 'lines', name=None, text = []):\n",
    "        coef_text = f\"y = {self.coef_}x\"\n",
    "        intercept_text = f\" + {self.intercept_}\"\n",
    "        default_text = coef_text + intercept_text if self.intercept_ else coef_text\n",
    "        text = name or default_text\n",
    "        return {'x': self.x_values, 'y': self.predict(), 'mode': mode, 'name': name, 'text': text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Optimizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we left in the beginning components to our Optimizer class.  \n",
    "\n",
    "Let's take a look at the `__init__` function.  Note that the optimizer begins by taking in our data of the actual `x_values` and `y_values`.  It also takes in the y-intercept value.  This is because we will not tackle the more complicated problem of having our Optimizer find both parameters, it will only find the coefficient.\n",
    "\n",
    "The `start`, `stop` and `step_count` parameters will be explained further down below.  So will the `steps` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, x_values, y_values, intercept, start, stop, step_count):\n",
    "        self.x_values = x_values\n",
    "        self.y_values = y_values\n",
    "        self.intercept = intercept\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.step_count = step_count\n",
    "        \n",
    "    def steps(self):\n",
    "        step_size = (self.stop - self.start)/self.step_count\n",
    "        self.steps = []\n",
    "        for count in list(range(0, self.step_count)):\n",
    "            self.steps.append(self.start + count*step_size)\n",
    "        return self.steps\n",
    "    \n",
    "    def set_hypotheses(self):\n",
    "        coefs = self.steps\n",
    "        self.hypotheses = [Hypothesis(coef, self.intercept, self.x_values) for coef in coefs]\n",
    "    \n",
    "    def set_losses(self):\n",
    "        self.losses = [Loss(hypothesis, self.x_values, self.y_values) for hypothesis in self.hypotheses]\n",
    "        \n",
    "    def find_min(self):\n",
    "        minimum_loss = self.losses[0]\n",
    "        for loss in self.losses[1:]:\n",
    "            if loss.rss() < minimum_loss.rss():\n",
    "                minimum_loss = loss\n",
    "        return minimum_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the goal of our optimizer is to find the value for our coefficient parameter that minimize the our `rss`.  The way that we'll do this is to create a list of Hypothesis instances, each with a sequential value of `m`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our first Hypothesis could be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 153\n",
    "x_values = [800, 1500, 2000, 3500, 4000]\n",
    "coef = .01\n",
    "\n",
    "hyp = Hypothesis(coef, intercept, x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the second Hypothesis instance would have the same data except a slightly different coefficient parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = .02\n",
    "hyp = Hypothesis(coef, intercept, x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan is to create a list of these Hypothesis instances, and then use our Loss class to find the hypothesis with the smallest rss score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's where our steps method enters the picture: it generates a list of $m$ values to then pass through to create a list of Hypothesis instances.  To do so we just need to tell our Optimizer of a starting point, a stopping point, and a step size.  Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 153\n",
    "coef = .01\n",
    "start = 0\n",
    "stop = 1\n",
    "step_number = 100\n",
    "x_values = [800, 1500, 2000, 3500, 4000]\n",
    "outcomes = [330, 780, 1130, 1310, 1780]\n",
    "optimizer = Optimizer(x_values, outcomes, intercept, start, stop, step_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_values': [800, 1500, 2000, 3500, 4000],\n",
       " 'y_values': [330, 780, 1130, 1310, 1780],\n",
       " 'intercept': 153,\n",
       " 'start': 0,\n",
       " 'stop': 1,\n",
       " 'step_count': 100}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a list of steps.  Each one of these could be a different value for `m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.steps()[0:10]\n",
    "# [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our steps to create a number of Hypothesis instances, each with the same input values and y-intercept, but a different coefficient.  If you look at the `set_hypotheses` step, you'll that it is responsible for creating a list of `hypotheses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.set_hypotheses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of hypotheses, each with a different coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Hypothesis at 0x10ad17ef0>,\n",
       " <__main__.Hypothesis at 0x10ad17a58>,\n",
       " <__main__.Hypothesis at 0x10ad17da0>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.hypotheses[0:3]\n",
    "# [<__main__.Hypothesis at 0x10bbe12b0>,\n",
    "#  <__main__.Hypothesis at 0x10bbe1630>,\n",
    "#  <__main__.Hypothesis at 0x10bbe1470>]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[hyp.coef_ for hyp in optimizer.hypotheses[0:9]]\n",
    "# [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have an optimizer class, that can create a number of Hypotheses instances, each with a different coefficient.  What's left is to calculate the `rss` for each of these Hypotheses instances, and find the instance with the lowest `rss`.  \n",
    "\n",
    "Our hypotheses instances don't have the capability to calculate the `rss`.  That functionality lies with our `Loss` instances.  First copy and paste the Loss class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss class\n",
    "class Loss():\n",
    "    def __init__(self, hypothesis, x_values, y_values):\n",
    "        self.hypothesis = hypothesis\n",
    "        self.x_values = x_values\n",
    "        self.y_values = y_values\n",
    "        \n",
    "    def errors(self):\n",
    "        predicted_values = self.hypothesis.predict()\n",
    "        zipped_list = zip(self.y_values,predicted_values)\n",
    "        return [actual-expected for actual,expected in zipped_list]\n",
    "    \n",
    "    def squared_errors(self):\n",
    "        error_list = self.errors()\n",
    "        return [error**2 for error in error_list]\n",
    "    \n",
    "    def rss(self):\n",
    "        squared_error_list = self.squared_errors()\n",
    "        return sum(squared_error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So in the `set_losses` method, use each of the hypothesis instances to create a list of Loss instances, and assign these loss instances to an attribute called `losses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Loss at 0x10ad3cd30>,\n",
       " <__main__.Loss at 0x10ad3c7f0>,\n",
       " <__main__.Loss at 0x10ad3ce10>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.set_losses()\n",
    "optimizer.losses[0:3]\n",
    "\n",
    "# [<__main__.Loss at 0x10bc185c0>,\n",
    "#  <__main__.Loss at 0x10bc185f8>,\n",
    "#  <__main__.Loss at 0x10bc18630>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the losses should store the related hypothesis instances and should also store the x_values and y_values of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hypothesis': <__main__.Hypothesis at 0x10ad17ef0>,\n",
       " 'x_values': [800, 1500, 2000, 3500, 4000],\n",
       " 'y_values': [330, 780, 1130, 1310, 1780]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.losses[0].__dict__\n",
    "\n",
    "# {'hypothesis': <__main__.Hypothesis at 0x10bbe12b0>,\n",
    "#  'x_values': [800, 1500, 2000, 3500, 4000],\n",
    "#  'y_values': [330, 780, 1130, 1310, 1780]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[loss.hypothesis.coef_ for loss in optimizer.losses[0:9]]\n",
    "\n",
    "# [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a method called `find_min` that returns the Loss object with the lowest `rss`.  From there, we can find the related Hypothesis instance, and it's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coef_': 0.39, 'intercept_': 153, 'x_values': [800, 1500, 2000, 3500, 4000]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = optimizer.find_min()\n",
    "loss.hypothesis.__dict__\n",
    "\n",
    "# {'coef_': 0.39, 'intercept_': 153, 'x_values': [800, 1500, 2000, 3500, 4000]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the hypothesis that minimizes our RSS the hypothesis with a coefficient of .39.  This is within one one-hundredth of what we calculated with SKLearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll build out the component that finds the hypothesis function that minimizes the output of our loss component.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
